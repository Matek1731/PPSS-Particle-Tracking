{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of dnn_tracking_2D_mdn_multimod_det_plots.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ud-TQ2ee_1MM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "eeb091f6-7da2-455c-9bc9-a8841dcc84d1"
      },
      "source": [
        "'''\n",
        "Reconstructs straight tracks in 3D (with noise and inefficiency)\n",
        "Using google TPUs to speed up the training\n",
        "'''\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "\n",
        "!pip install keras_sequential_ascii\n",
        "!pip install git+git://github.com/cpmpercussion/keras-mdn-layer.git#egg=keras-mdn-layer\n",
        "\n",
        "\n",
        "#import ROOT\n",
        "#from ROOT import TMVA, TFile, TTree, TCut, TChain, TString, TF1, TGraphErrors, TFile, TNtuple, TCanvas\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# Package imports\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "import scipy.stats as stats\n",
        "import scipy as sp\n",
        "import logging\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "import pprint\n",
        "\n",
        "import keras\n",
        "\n",
        "import mdn\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, RepeatVector, LSTM, TimeDistributed\n",
        "from keras.layers import Conv3D, MaxPooling3D\n",
        "from keras.layers import Reshape\n",
        "from keras.models import Sequential\n",
        "from keras_sequential_ascii import sequential_model_to_ascii_printout\n",
        "from keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "from keras.optimizers import SGD, RMSprop, Adam, Nadam\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "import sys\n",
        "import time\n",
        "\n",
        "np.random.seed(2348)\n",
        "\n",
        "batch_size = 5000\n",
        "batch_size_NN = 2*128 #2*128\n",
        "epochs = 512 #512\n",
        "pre_epochs =  16 #32 epochs for pre-traing using rms optimizer\n",
        "post_epochs = 16 #64 post-training using SGD\n",
        "steps_per_epoch = 2*8 #4*8\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 400, 12 #28, 28"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras_sequential_ascii in /usr/local/lib/python3.6/dist-packages (0.1.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras_sequential_ascii) (2.2.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras_sequential_ascii) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras_sequential_ascii) (1.3.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras_sequential_ascii) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras_sequential_ascii) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras_sequential_ascii) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras_sequential_ascii) (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras_sequential_ascii) (1.16.4)\n",
            "Requirement already satisfied: keras-mdn-layer from git+git://github.com/cpmpercussion/keras-mdn-layer.git#egg=keras-mdn-layer in /usr/local/lib/python3.6/dist-packages (0.2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAzAz_c-HhOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "####################################################################################################\n",
        "\n",
        "# Data parameters\n",
        "det_width = img_rows  # stupid, to be changed\n",
        "det_depth = img_cols\n",
        "det_shape = (det_depth, det_width)#, det_width)\n",
        "\n",
        "canals=3 #number of detector channels\n",
        "canal_depth=80 #empty channel depth\n",
        "canal_det=int(det_depth/canals) #detector channel depth\n",
        "\n",
        "\n",
        "# Number of tracks in each event follows Poisson distribution\n",
        "mean_tracks = 2\n",
        "max_tracks  = 2\n",
        "\n",
        "#probability of noise and the efficiency<1\n",
        "prob_noise = 0.01\n",
        "efficiency = 0.90\n",
        "\n",
        "#slope multiplicator - reduce the slope to slope_mult*max_slope\n",
        "slope_mult = 1\n",
        "\n",
        "#MDN parameters\n",
        "mdn_out=2\n",
        "mdn_mix=3\n",
        "\n",
        "y_new = []\n",
        "\n",
        "################################ matplotlib-hep ########################################### \n",
        "\n",
        "\n",
        "def calc_nbins(x, maximum=150):\n",
        "    n =  (max(x) - min(x)) / (2 * len(x)**(-1/3) * (np.percentile(x, 75) - np.percentile(x, 25)))\n",
        "    print(f\"bins_proposed: {n}\")\n",
        "    return np.floor(min(n, maximum))\n",
        "\n",
        "def poisson_limits(N, kind, confidence=0.6827):\n",
        "    alpha = 1 - confidence\n",
        "    upper = np.zeros(len(N))\n",
        "    lower = np.zeros(len(N))\n",
        "    if kind == 'gamma':\n",
        "        lower = stats.gamma.ppf(alpha / 2, N)\n",
        "        upper = stats.gamma.ppf(1 - alpha / 2, N + 1)\n",
        "    elif kind == 'sqrt':\n",
        "        err = np.sqrt(N)\n",
        "        lower = N - err\n",
        "        upper = N + err\n",
        "    else:\n",
        "        raise ValueError('Unknown errorbar kind: {}'.format(kind))\n",
        "    # clip lower bars\n",
        "    lower[N==0] = 0\n",
        "    return N - lower, upper - N\n",
        "\n",
        "def histpoints(x, bins=None, xerr=None, yerr='gamma', normed=False, **kwargs):\n",
        "    \"\"\"\n",
        "    Plot a histogram as a series of data points.\n",
        "    Compute and draw the histogram of *x* using individual (x,y) points\n",
        "    for the bin contents.\n",
        "    By default, vertical poisson error bars are calculated using the\n",
        "    gamma distribution.\n",
        "    Horizontal error bars are omitted by default.\n",
        "    These can be enabled using the *xerr* argument.\n",
        "    Use ``xerr='binwidth'`` to draw horizontal error bars that indicate\n",
        "    the width of each histogram bin.\n",
        "    Parameters\n",
        "    ---------\n",
        "    x : (n,) array or sequence of (n,) arrays\n",
        "        Input values. This takes either a single array or a sequence of\n",
        "        arrays, which are not required to be of the same length.\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    if bins is None:\n",
        "        bins = int(calc_nbins(x))\n",
        "\n",
        "    h, bins = np.histogram(x, bins=bins)\n",
        "    width = bins[1] - bins[0]\n",
        "    center = (bins[:-1] + bins[1:]) / 2\n",
        "    area = sum(h * width)\n",
        "\n",
        "    if isinstance(yerr, str):\n",
        "        yerr = poisson_limits(h, yerr)\n",
        "\n",
        "    if xerr == 'binwidth':\n",
        "        xerr = width / 2\n",
        "\n",
        "    if normed:\n",
        "        h = h / area\n",
        "        yerr = yerr / area\n",
        "        area = 1.\n",
        "\n",
        "    if not 'color' in kwargs:\n",
        "        kwargs['color'] = 'black'\n",
        "\n",
        "    if not 'fmt' in kwargs:\n",
        "        kwargs['fmt'] = 'o'\n",
        "\n",
        "    plt.errorbar(center, h, xerr=xerr, yerr=yerr, **kwargs)\n",
        "\n",
        "    return center, (yerr[0], h, yerr[1]), area\n",
        "\n",
        "def make_split(ratio, gap=0.12):\n",
        "    import matplotlib.pyplot as plt\n",
        "    from matplotlib.gridspec import GridSpec\n",
        "    from matplotlib.ticker import MaxNLocator\n",
        "    cax = plt.gca()\n",
        "    box = cax.get_position()\n",
        "    xmin, ymin = box.xmin, box.ymin\n",
        "    xmax, ymax = box.xmax, box.ymax\n",
        "    gs = GridSpec(2, 1, height_ratios=[ratio, 1 - ratio], left=xmin, right=xmax, bottom=ymin, top=ymax)\n",
        "    gs.update(hspace=gap)\n",
        "\n",
        "    ax = plt.subplot(gs[0])\n",
        "    plt.setp(ax.get_xticklabels(), visible=False)\n",
        "    bx = plt.subplot(gs[1], sharex=ax)\n",
        "\n",
        "    return ax, bx\n",
        "\n",
        "def plot_pull(data, func,title):\n",
        "\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "    ax, bx = make_split(0.8)\n",
        "\n",
        "    plt.sca(ax)\n",
        "\n",
        "    x, y, norm = histpoints(data)\n",
        "\n",
        "    lower, upper = ax.get_xlim()\n",
        "\n",
        "    xs = np.linspace(-max(abs(lower), upper), max(abs(lower), upper), 200)\n",
        "    plt.plot(xs, norm * func(xs), 'b-')\n",
        "\n",
        "    #plt.gca().yaxis.set_major_locator(MaxNLocator(prune='lower'))\n",
        "    plt.title(title)\n",
        "    plt.sca(bx)\n",
        "\n",
        "    resid = y[1] - norm * func(x)\n",
        "    err = np.zeros_like(resid)\n",
        "    err[resid >= 0] = y[0][resid >= 0]\n",
        "    err[resid < 0] = y[2][resid < 0]\n",
        "\n",
        "    pull = resid / err\n",
        "\n",
        "    plt.errorbar(x, pull, yerr=1, color='k', fmt='o')\n",
        "    plt.ylim(-5, 5)\n",
        "    plt.axhline(0, color='b')\n",
        "\n",
        "    plt.sca(ax)\n",
        "\n",
        "    return ax, bx\n",
        "  \n",
        "\n",
        "# ### Functions for toy data generation\n",
        "\n",
        "\n",
        "def simulate_straight_track(b2x, bx, det_shape):\n",
        "    \"\"\"\n",
        "    Simulate detector data for one straight track.\n",
        "    Parameters:\n",
        "        b2x, b2y: track last layer intercept\n",
        "        bx, by: track first-layer intercept parameter (detector entry point)\n",
        "        det_shape: tuple of detector shape: (width, width, depth)\n",
        "    Returns:\n",
        "        ndarray of binary detector data for one track.\n",
        "    \"\"\"\n",
        "    x = np.zeros(det_shape)\n",
        "    idz = np.arange(det_shape[0])\n",
        "    idz += [canal_depth*int(i/canal_det) for i in idz]\n",
        "    #print(idz)\n",
        "    hitsx = (idz*(b2x-bx)/(canal_depth*(canals-1)+det_shape[0]) + bx +0.5).astype(int)  ## +0.5 to get rounding, not clipping\n",
        "    #hitsy = (idz*(b2y-by)/det_depth + by +0.5).astype(int)  ## +0.5 to get rounding, not clipping\n",
        "    # implement hit efficiency\n",
        "    idz -= [canal_depth*int(i/canal_det) for i in range(det_shape[0])]\n",
        "\n",
        "    for i in range(det_shape[0]):\n",
        "        if (np.random.random() > efficiency):\n",
        "            hitsx[i]=0\n",
        "            #hitsy[i]=0\n",
        "    #print (\"HITS = \",hits)\n",
        "    valid = (hitsx > 0) & (hitsx < det_shape[1]) #& (hitsy > 0) & (hitsy < det_shape[1])\n",
        "    x[idz[valid], hitsx[valid]] = 1\n",
        "    return x\n",
        "\n",
        "# Generator for single-track events\n",
        "def gen_tracks(batch_size=batch_size, det_shape=det_shape):\n",
        "    \"\"\"Arguments:\n",
        "         batch_size: number of events to yield for each call\n",
        "       Yields: batches of training data for use with the keras fit_generator function\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        # Entry and exit points are randomized\n",
        "        bsx = np.random.random_sample(size=batch_size)*(det_width-6)+3\n",
        "        b2sx = np.random.random_sample(size=batch_size)*(det_width-6)+3\n",
        "        # restrict the slope of a track to slope_mult*slope\n",
        "        b2sx = bsx + slope_mult*(b2sx-bsx)\n",
        "        \n",
        "#         bsy = np.random.random_sample(size=batch_size)*(det_width-6)+3\n",
        "#         b2sy = np.random.random_sample(size=batch_size)*(det_width-6)+3\n",
        "#         # restrict the slope of a track to slope_mult*slope\n",
        "#         b2sy = bsy + slope_mult*(b2sy-bsy)\n",
        "        \n",
        "        tracks = np.zeros((batch_size, 1, det_depth, det_width))\n",
        "        params = zip(bsx, b2sx)#, bsy, b2sy )\n",
        "        for i, (pbx, pb2x) in enumerate(params):\n",
        "            tracks[i,0] = simulate_straight_track(pb2x, pbx, det_shape)\n",
        "###        targets = zip(bsx, msx, bsy, msy)\n",
        "        targets = list(zip(bsx/det_width, b2sx/det_width))#(b2sx-bsx)/(canal_depth*(canals-1)+det_shape[0])/slope_mult)) \n",
        "                      #bsy/det_width, (b2sy-bsy)/det_depth/slope_mult ))  # save b and b2 as targets\n",
        "        targets = np.asarray(targets)\n",
        "        yield tracks, targets\n",
        "\n",
        "# Generator for multi-track events.\n",
        "# Each event contains exactly n_tracks tracks.\n",
        "# The target track parameters are sorted in increasing order of X intercept.\n",
        "def gen_n_tracks(batch_size=batch_size, det_shape=det_shape, n_tracks=mean_tracks, train = True):\n",
        "    gen_single = gen_tracks(batch_size=n_tracks, det_shape=det_shape)\n",
        "    while True:\n",
        "        batch_events = np.zeros((batch_size, 1, det_depth, det_width))\n",
        "#        batch_targets = -np.ones((batch_size, n_tracks, 4))\n",
        "        if train:\n",
        "          batch_targets = -np.ones((batch_size, 1, 2))\n",
        "        else:\n",
        "          batch_targets = -np.ones((batch_size, n_tracks, 2))\n",
        "\n",
        "        #print(\"batch_targets 0 \",batch_targets.shape)\n",
        "        for n in range(batch_size):\n",
        "            tracks,targets = gen_single.__next__()\n",
        "            batch_events[n,0] = np.clip( sum( tracks ), 0, 1)\n",
        "            \n",
        "            event_targets = np.asarray(targets)\n",
        "            #print(event_targets.shape)\n",
        "            #event_targets = event_targets[:,:,0]\n",
        "            #print(\"event_targets\",event_targets,targets)\n",
        "            if train:\n",
        "              batch_targets[n] = event_targets[0]#[event_targets[:,0].argsort()] # sort by first column\n",
        "            else:\n",
        "              batch_targets[n] = event_targets[event_targets[:,0].argsort()] # sort by first column\n",
        "\n",
        "            #add empty rows to get the size of max_tracks\n",
        "            #for k in range(max_tracks-n_tracks):\n",
        "            ###batch_targets[n] = event_targets  #do not sort\n",
        "            #print(\"batch_events \",batch_events.shape)\n",
        "            #print(\"batch_targets \",batch_targets.shape)\n",
        "        yield batch_events, batch_targets\n",
        "\n",
        "# Generator for multi-track events.\n",
        "# Each event contains up to max_tracks tracks.\n",
        "# The target track parameters are sorted in increasing order of intercept.\n",
        "def gen_multi_tracks(batch_size=batch_size, det_shape=det_shape, mean_tracks=mean_tracks):\n",
        "    gen_single = gen_tracks(batch_size=max_tracks, det_shape=det_shape)\n",
        "    while True:\n",
        "        batch_events = np.zeros((batch_size, 1, det_depth, det_width))\n",
        "        batch_targets = -np.ones((batch_size, max_tracks, 4))\n",
        "        for n in range(batch_size):\n",
        "            num_tracks = min( max_tracks, np.random.poisson(mean_tracks) )\n",
        "            tracks,targets = gen_single.__next__()\n",
        "            batch_events[n,0] = np.clip( sum( tracks[:num_tracks] ), 0, 1)\n",
        "            event_targets = np.asarray(targets[:num_tracks])\n",
        "            batch_targets[n,:num_tracks] = event_targets[event_targets[:,0].argsort()] # sort by first column            \n",
        "        yield batch_events, batch_targets\n",
        "\n",
        "\n",
        "def gen_noise(batch_size=batch_size, det_shape=det_shape, prob_noise=prob_noise, dims = 3):\n",
        "\n",
        "        batch_events = np.zeros((batch_size, 1, det_depth, det_width))\n",
        "        for n in range(batch_size):\n",
        "        \n",
        "            for i in range(det_depth):\n",
        "               for j in range(det_width):\n",
        "                  if dims == 3:\n",
        "                    for k in range(det_width):\n",
        "                       if np.random.random()<prob_noise:\n",
        "                          batch_events[n,0,i,j,k]=1\n",
        "                  elif dims == 2:\n",
        "                      if np.random.random()<prob_noise:\n",
        "                            batch_events[n,0,i,j]=1\n",
        "\n",
        "\n",
        "        yield batch_events\n",
        "\n",
        "\n",
        "####################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################################\n",
        "\n",
        "def get_activations(model, model_inputs, print_shape_only=False, layer_name=None):\n",
        "    print('----- activations -----')\n",
        "    activations = []\n",
        "    inp = model.input\n",
        "\n",
        "    model_multi_inputs_cond = True\n",
        "    if not isinstance(inp, list):\n",
        "        # only one input! let's wrap it in a list.\n",
        "        inp = [inp]\n",
        "        model_multi_inputs_cond = False\n",
        "\n",
        "    outputs = [layer.output for layer in model.layers if\n",
        "               layer.name == layer_name or layer_name is None]  # all layer outputs\n",
        "\n",
        "\n",
        "\n",
        "    funcs = [K.function(inp + [K.learning_phase()], [out]) for out in outputs]  # evaluation functions\n",
        "\n",
        "    if model_multi_inputs_cond:\n",
        "        list_inputs = []\n",
        "        list_inputs.extend(model_inputs)\n",
        "        list_inputs.append(0.)\n",
        "    else:\n",
        "        list_inputs = [model_inputs, 0.]\n",
        "\n",
        "    # Learning phase. 0 = Test mode (no dropout or batch normalization)\n",
        "    # layer_outputs = [func([model_inputs, 0.])[0] for func in funcs]\n",
        "    layer_outputs = [func(list_inputs)[0] for func in funcs]\n",
        "    for layer_activations in layer_outputs:\n",
        "        activations.append(layer_activations)\n",
        "        if print_shape_only:\n",
        "            print(layer_activations.shape)\n",
        "        else:\n",
        "            print(layer_activations)\n",
        "    return activations\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#####################################################################################\n",
        "\n",
        "def train_model(model, prob_noise=prob_noise,epochs=epochs):\n",
        "\n",
        "  print(\"Training with ADAM optimizer, efficiency, prob_noise = \",efficiency,prob_noise)\n",
        "  \n",
        "  #recompile the model with Adam optimizer \n",
        "  #model.compile(optimizer='Adam', loss=mdn.get_mixture_loss_func(mdn_out,mdn_mix))\n",
        "  \n",
        "  #model.load_weights(\"model.h5\")\n",
        "  print(\"Model loaded from disk\") \n",
        "\n",
        "  # patient early stopping\n",
        "  es_noise = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=12)  # 'val_loss'\n",
        "  mc_noise = ModelCheckpoint('best_model.h5', monitor='loss', mode='min', verbose=1,\n",
        "                     save_best_only=True)  # 'val_loss'\n",
        "\n",
        "  \n",
        "  \n",
        "\n",
        "  #train with fit_generator - generate events on flight\n",
        "  history=model.fit_generator(data_generator(prob_noise=prob_noise, n_tracks=mean_tracks),\n",
        "                            steps_per_epoch=steps_per_epoch,\n",
        "                            #validation_data=data_generator(), \n",
        "                            #validation_steps=steps_per_epoch,\n",
        "                            epochs = epochs, callbacks=[es_noise\n",
        "#                                                         , mc_noise\n",
        "                                                       ])\n",
        "\n",
        "  \n",
        "  # serialize weights to HDF5\n",
        "  model.save_weights(\"model.h5\")\n",
        "  print(\"Saved model to disk\")\n",
        "  \n",
        "\n",
        "#   # load the saved model\n",
        "#   model = load_model('best_model.h5')\n",
        "\n",
        "  # summarize history for loss\n",
        "  plt.plot(history.history['loss'])\n",
        "  ###plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.xlim(3, None)\n",
        "  plt.ylim(ymax = 1.2*history.history['loss'][min(len(history.history['loss']),3)-1], ymin = 0)\n",
        "  print(\"history\",history.history['loss'])\n",
        "\n",
        "  auxName = 'training_'+str(efficiency)+'_'+str(prob_noise)+'.png'\n",
        "  plt.savefig(auxName)\n",
        "  \n",
        "  plt.show()\n",
        "  plt.clf()\n",
        "  \n",
        "  \n",
        "  \n",
        "# #     recompile the model with SGD optimizer \n",
        "#   model.compile(optimizer='SGD', loss=mdn.get_mixture_loss_func(mdn_out,mdn_mix))\n",
        "\n",
        "\n",
        "#   model.load_weights(\"model.h5\")\n",
        "#   print(\"Model loaded from disk\")\n",
        "  \n",
        "\n",
        "#   #train with fit_generator - generate events on flight\n",
        "#   history=model.fit_generator(data_generator(prob_noise=prob_noise, n_tracks=mean_tracks),\n",
        "#                             steps_per_epoch=steps_per_epoch,\n",
        "#                             #validation_data=data_generator(), \n",
        "#                             #validation_steps=steps_per_epoch,\n",
        "#                             epochs = post_epochs, callbacks=[es_noise\n",
        "# #                                                              , mc_noise\n",
        "#                                                             ])\n",
        "\n",
        "  \n",
        "#       # serialize weights to HDF5\n",
        "#   model.save_weights(\"model.h5\")\n",
        "#   print(\"Saved model to disk\")\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "# #   # load the saved model\n",
        "# #   model = load_model('best_model.h5')\n",
        "  \n",
        "#   # summarize history for loss\n",
        "#   plt.plot(history.history['loss'])\n",
        "#   ###plt.plot(history.history['val_loss'])\n",
        "#   plt.title('model loss')\n",
        "#   plt.ylabel('loss')\n",
        "#   plt.xlabel('epoch')\n",
        "#   plt.legend(['train', 'test'], loc='upper left')\n",
        "#   plt.xlim(3, None)\n",
        "#   plt.ylim(ymax = 1.2*history.history['loss'][min(len(history.history['loss']),3)-1], ymin = 0)\n",
        "#   print(\"history\",history.history['loss'])\n",
        "\n",
        "  auxName = 'trainingSGD_'+str(efficiency)+'_'+str(prob_noise)+'.png'\n",
        "  plt.savefig(auxName)\n",
        "  \n",
        "  plt.show()\n",
        "  plt.clf()\n",
        "  \n",
        "\n",
        "######################################################################################################\n",
        "\n",
        "def generate_testdata(prob_noise = prob_noise):\n",
        "\n",
        "  print(\"Generate test data, efficiency, prob_noise = \",efficiency,prob_noise)\n",
        "\n",
        "  ###x_train, y_train = gen_n_tracks(batch_size=batch_size).__next__()\n",
        "  x_test, y_test = gen_n_tracks(batch_size=batch_size, n_tracks=mean_tracks, train = False).__next__()\n",
        "  \n",
        "  #y_test = y_test[:, :, 0:2]\n",
        "\n",
        "  # add noise\n",
        "  #noise_test = gen_noise(batch_size=batch_size, prob_noise=prob_noise).__next__()\n",
        "  #x_test  = x_test+noise_test\n",
        "  \n",
        "  noise_test = gen_noise(batch_size=batch_size,prob_noise=prob_noise, dims = 2).__next__()\n",
        "  x_test = x_test+noise_test\n",
        "    \n",
        "  x_test = np.clip(x_test, 0, 1)\n",
        " \n",
        "  print(x_test.size)\n",
        "\n",
        "  x_test = x_test.astype('int')\n",
        "  #clip to 1\n",
        "  x_test = np.clip(x_test, 0, 1)\n",
        "\n",
        "  return x_test, y_test\n",
        "\n",
        "######################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "def plot_sample():\n",
        "\n",
        "  #plot sample events\n",
        "  print(\"plotting sample events, efficiency, prob_noise = \",efficiency,prob_noise)\n",
        "\n",
        "  for n in range(1):\n",
        "    for k in range(int(100*(prob_noise+0.10))):\n",
        "      rnumber = np.random.randint(0,len(x_test))\n",
        "      x,z = x_test[rnumber,0].nonzero()\n",
        "      from mpl_toolkits.mplot3d import Axes3D\n",
        "      fig = plt.figure()\n",
        "      ax = fig.add_subplot(111)\n",
        "      ax.scatter(x, z, c= 'red', s=5, alpha=0.9)\n",
        "    \n",
        "    \n",
        "      ax.set_xlabel('X')\n",
        "      #ax.set_ylabel('Y')\n",
        "      ax.set_ylabel('Z (depth)')\n",
        "  \n",
        "    \n",
        "      for j in range(len(y_test[rnumber])):\n",
        "      # Data for a three-dimensional line\n",
        "         #print(\"index \",k)\n",
        "         #print(\"ytrain \", y_train[rnumber])\n",
        "         zline = np.linspace(0, det_depth, 100)\n",
        "         xline = y_test[rnumber,j,0]*det_width+y_test[rnumber,j,1]*slope_mult*zline \n",
        "         #yline = y_test[rnumber,j,2]*det_width+y_test[rnumber,j,3]*slope_mult*zline \n",
        "###       xline = 100*y_test[rnumber,k,0]+100/slope_scale*y_test[rnumber,k,1]*zline\n",
        "###       yline = 100*y_test[rnumber,k,2]+100/slope_scale*y_test[rnumber,k,3]*zline\n",
        "         ax.plot(xline, zline,  'red', alpha=0.5)\n",
        "    \n",
        "      #ax.view_init(elev=35., azim=45)\n",
        "      ax.scatter([0,det_width],[0,det_depth], c= 'blue', s=0)\n",
        "\n",
        "      # set axes range\n",
        "      ax.set_xlim(0,det_width)\n",
        "      ax.set_ylim(0,det_depth)\n",
        "      #ax.set_xlim3d(0,det_depth)\n",
        "      #ax.axis('equal')\n",
        "\n",
        "      print('event_'+str(efficiency)+\"_\"+str(prob_noise)+\"_\"+str(k)+'.png')\n",
        "      plt.savefig('event_'+str(efficiency)+\"_\"+str(prob_noise)+\"_\"+str(k)+'.png')\n",
        "      plt.show()\n",
        "  \n",
        "#  plt.imshow(x_train[np.random.randint(0,len(x_train)),0])\n",
        "#  plt.show()\n",
        "#plt.savefig('event.png')\n",
        "#plt.clf()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#sys.exit(\"Stopping here\")\n",
        "\n",
        "############################################################################################\n",
        "\n",
        "def predict(model):\n",
        "\n",
        "  print(\"Predict with trained NN, efficiency, prob_noise = \",efficiency,prob_noise)\n",
        "\n",
        "  start_pattern_time = time.time()\n",
        "\n",
        "  if 'COLAB_TPU_ADDR'  in os.environ:\n",
        "  # converting tpu_model to the cpu_model\n",
        "   model = model.sync_to_cpu()\n",
        "\n",
        "\n",
        "# make a prediction\n",
        "#batch_size=2\n",
        "  print(\"x_test.shape\", x_test.shape)\n",
        "\n",
        "  y_new = model.predict(x_test)\n",
        "\n",
        "  print (model.summary())\n",
        "  print(\"y_new.shape\", y_new.shape)\n",
        "\n",
        "  end_pattern_time = time.time()\n",
        "\n",
        "  return y_new\n",
        "\n",
        "#########################################################################################\n",
        "\n",
        "def plot_predict(y_new = y_new):\n",
        "\n",
        "  print(\"Plot fitted parameters, efficiency, prob_noise = \",efficiency,prob_noise)\n",
        "\n",
        "#   print(\"y_new shape :: \",y_new.shape)\n",
        "#   print(y_test.shape)\n",
        "  # plot fitted parameters\n",
        "  y0_true = []\n",
        "  y0_pred = []\n",
        "  y0_pred_std = []\n",
        "\n",
        "  best_fitted_index = []\n",
        "\n",
        "  for i in range(len(x_test)):\n",
        "    if len(y_test[i])!=1:\n",
        "      y0_true.append((y_test[i])\n",
        "                   [np.random.randint(0,len(y_test[i])-1)]#0\n",
        "                  )\n",
        "    else:\n",
        "      y0_true.append((y_test[i])[0])\n",
        "#     if i < 6:\n",
        "#       print(\"(y_test[i])[0].shape: \", (y_test[i])[0].shape)\n",
        "#     y0_pred.append((y_new[i])[0])\n",
        "#     if i<6:\n",
        "#         print (\"true \",y_test[i])\n",
        "#         print (\"pred \",y_new[i])\n",
        "#         print (y0_true[i], y0_pred[i])\n",
        "    index_distances = []\n",
        "    for j in range(mdn_mix):\n",
        "#       print(\"y_new[i, j:j+2]: \", y_new[i, j:j+2])\n",
        "#       print(\"y0_true[0]: \", y0_true[i])\n",
        "#       print(\"y_new: \", y_new)\n",
        "#       if i < 6:\n",
        "#         print(\"Sumy kwadratÃ³w (tablica): \", (y_new[i, 2*j:2*j+2] - y0_true[i]) ** 2)\n",
        "      sum_of_squares = np.sum((y_new[i, 2*j:2*j+2] - y0_true[i]) ** 2)\n",
        "#       if i < 6:\n",
        "#         print(\"sum_of_squares: \", sum_of_squares)\n",
        "      index_distances.append(sum_of_squares)\n",
        "    best_fitted_index.append(np.argmin(index_distances))\n",
        "#     if i < 6:\n",
        "#       print(\"best_fitted_index: \", best_fitted_index)\n",
        "    \n",
        "#       print(\"y_new[i][best_fitted_index[i]:best_fitted_index[i]+2]: \", y_new[i][best_fitted_index[i]:best_fitted_index[i]+2])\n",
        "#       print(\"y_new[i][best_fitted_index[i] + 2 * mdn_mix : best_fitted_index[i] + 2 + 2 * mdn_mix]: \", y_new[i][best_fitted_index[i] + 2 * mdn_mix : best_fitted_index[i] + 2 + 2 * mdn_mix])\n",
        "\n",
        "    y0_pred.append(y_new[i][2*best_fitted_index[i]:2*best_fitted_index[i]+2])\n",
        "    y0_pred_std.append(y_new[i][2*best_fitted_index[i] + 2 * mdn_mix : 2*best_fitted_index[i] + 2 + 2 * mdn_mix])\n",
        "#     if i < 6:\n",
        "#       print(\"y0_pred[i]: \", y0_pred[i])\n",
        "#       print(\"y0_pred_std[i]: \", y0_pred_std[i])\n",
        "\n",
        "    \n",
        "      \n",
        "    \n",
        "#   print(np.asarray(y0_true)[:, 1])\n",
        "#   print(np.asarray(y0_pred)[:, 1])\n",
        "\n",
        "#   print(\"shapes: \", np.asarray(y0_true).shape,  np.asarray(y0_pred).shape)\n",
        "#   print(\"tabs: \", np.asarray(y0_pred))\n",
        "\n",
        "   \n",
        "  plt.plot(np.asarray(y0_true)[:,:], np.asarray(y0_pred)[:, :], \".\")\n",
        "  plt.ylabel('predicted')\n",
        "  plt.xlabel('true')\n",
        "  print('params_scatter_'+str(efficiency)+'_'+str(prob_noise)+'.png')\n",
        "  plt.savefig('params_scatter_'+str(efficiency)+'_'+str(prob_noise)+'.png')\n",
        " \n",
        "  plt.show()\n",
        "  plt.clf()\n",
        "  \n",
        "   \n",
        "\n",
        "\n",
        "  # the histogram of the data\n",
        "  columns_true = list(zip(*y0_true)) #transpose rows to columns\n",
        "  columns_pred = list(zip(*y0_pred)) #transpose rows to columns\n",
        "  #diff = columns_pred-columns_true\n",
        "  diff = tuple(np.subtract(columns_pred,columns_true) / list(zip(*y0_pred_std)))\n",
        "  \n",
        "  plt.figure(figsize=(20,10))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  n, bins, patches = plt.hist(diff[0], 50, normed=1, facecolor='green', alpha=0.30)\n",
        "  # Fit a normal distribution to the data:\n",
        "  mu, std = norm.fit(diff[0],loc=0)\n",
        "  new_diff_0 = diff[0][diff[0] < mu + 3*std]\n",
        "  new_diff_0 = new_diff_0[new_diff_0 > mu - 3*std]\n",
        "  #print(\"new_diff_0: \", new_diff_0)\n",
        "  mu, std = norm.fit(new_diff_0,loc=0)\n",
        "  # Plot the PDF.\n",
        "  xmin, xmax = plt.xlim()\n",
        "  x = np.linspace(xmin, xmax, 100)\n",
        "  p = norm.pdf(x, mu, std)\n",
        "  plt.title(\"$b_2$: $\\mu$ = %.2f,  $\\sigma$ = %.2f\" % (mu, std))\n",
        "  plt.plot(x, p, 'k', linewidth=2)\n",
        "  \n",
        "  func = lambda x: sp.stats.norm.pdf(x, mu, std)\n",
        "  plot_pull(diff[0],func,\"$b_1$: $\\mu_G$ = %.2f,  $\\sigma_G$ = %.2f\" % (mu, std))\n",
        "  \n",
        "  plt.subplot(1, 2, 2)\n",
        "  n, bins, patches = plt.hist(diff[1], 50, normed=1, facecolor='green', alpha=0.30)\n",
        "  # Fit a normal distribution to the data:\n",
        "  mu, std = norm.fit(diff[1],loc=0)\n",
        "  new_diff_1 = diff[1][diff[1] < mu + 3*std]\n",
        "  new_diff_1 = new_diff_1[new_diff_1 > mu - 3*std]\n",
        "  #print(\"new_diff_0: \", new_diff_0)\n",
        "  mu, std = norm.fit(new_diff_1,loc=0)\n",
        "  # Plot the PDF.\n",
        "  xmin, xmax = plt.xlim()\n",
        "  x = np.linspace(xmin, xmax, 100)\n",
        "  p = norm.pdf(x, mu, std)\n",
        "  plt.title(\"$b_1$: $\\mu_G$ = %.2f,  $\\sigma_G$ = %.2f\" % (mu, std))\n",
        "  plt.plot(x, p, 'k', linewidth=2)\n",
        "  \n",
        "  func = lambda x: sp.stats.norm.pdf(x,mu,std)\n",
        "  plot_pull(diff[1],func,\"$b_2$: $\\mu_G$ = %.2f,  $\\sigma_G$ = %.2f\" % (mu, std))\n",
        "\n",
        "\n",
        "  print('params_1_'+str(efficiency)+'_'+str(prob_noise)+'.png')\n",
        "  plt.savefig('params_1_'+str(efficiency)+'_'+str(prob_noise)+'.png')\n",
        "\n",
        "  plt.show()\n",
        "  plt.clf()\n",
        "   \n",
        "\n",
        "#########################################################################################\n",
        "\n",
        "def plot_predict_event(y_new = y_new):\n",
        "\n",
        "  \n",
        "  total_accuracy_nominator = 0\n",
        "  total_accuracy_denominator = 0\n",
        "  #plot sample events with true and reconstructed tracks\n",
        "  print(\"Plot events with predicted tracks, efficiency, prob_noise = \",efficiency,prob_noise)\n",
        "  \n",
        "  print(\"y_new shape :: \",y_new.shape)\n",
        "  \n",
        "\n",
        "  for n in range(8):\n",
        "    for k in range(int(100*(prob_noise+0.10))):\n",
        "      rnumber = np.random.randint(0,len(x_test))\n",
        "      x,z = x_test[rnumber,0].nonzero()\n",
        "      \n",
        "      x+=canal_depth*(x//canal_det)\n",
        "      \n",
        "      from mpl_toolkits.mplot3d import Axes3D\n",
        "      fig = plt.figure()\n",
        "      ax = fig.add_subplot(111)\n",
        "      ax.scatter(x, z, c= 'red', s=5, alpha=0.9)\n",
        "    \n",
        "      ax.set_xlabel('Z (depth)')\n",
        "      #ax.set_ylabel('Y')\n",
        "      ax.set_ylabel('X')\n",
        "  \n",
        "  \n",
        "   \n",
        "    \n",
        "      for j in range(mdn_mix):\n",
        "      # Data for a three-dimensional line\n",
        "         #print(\"index \",k)\n",
        "         #print(\"ytest \", y_test[rnumber])\n",
        "         zline = np.linspace(0, det_depth+canal_depth*(canals-1), 100)\n",
        "#          print(\"zline: \", zline)\n",
        "#          if j < 6:\n",
        "#           print(y_test.shape)\n",
        "\n",
        "\n",
        "#          xline = y_test[rnumber,j,0]*det_width+y_test[rnumber,j,1]*slope_mult*zline\n",
        "          # parametry b1, b2\n",
        "         if j < max_tracks:\n",
        "          xline2 = np.linspace(y_test[rnumber,j,0] * det_width, y_test[rnumber,j,1] * det_width, 100)\n",
        "          \n",
        "  \n",
        "         #yline = y_test[rnumber,j,2]*det_width+y_test[rnumber,j,3]*slope_mult*zline \n",
        "###       xline = 100*y_test[rnumber,k,0]+100/slope_scale*y_test[rnumber,k,1]*zline\n",
        "###       yline = 100*y_test[rnumber,k,2]+100/slope_scale*y_test[rnumber,k,3]*zline\n",
        "          ax.plot(zline, xline2, 'red', alpha=0.5)\n",
        "#          if j < 6:\n",
        "#           print(y_new.shape)\n",
        "#          xline = y_new[rnumber,2*j]*det_width+y_new[rnumber,2*j+1]*slope_mult*zline \n",
        "         #yline = y_new[rnumber,j,2]*det_width+y_new[rnumber,j,3]*slope_mult*zline  \n",
        "###       xline = 100*y_new[rnumber,k,0]+100/slope_scale*y_new[rnumber,k,1]*zline\n",
        "###       yline = 100*y_new[rnumber,k,2]+100/slope_scale*y_new[rnumber,k,3]*zline\n",
        "\n",
        "\n",
        "        # parametry b1, b2\n",
        "         if y_new[rnumber, j + 4*mdn_mix] > 0:\n",
        "          xline3 = np.linspace(y_new[rnumber,2*j] * det_width, y_new[rnumber,2*j+1] * det_width, 100)\n",
        "          ax.plot(zline, xline3, 'blue', alpha=0.5)\n",
        "         \n",
        "  \n",
        "  \n",
        "#          print(y_new.shape)\n",
        "          xline = y_new[rnumber,2*j]*det_width+y_new[rnumber,2*j+1]*slope_mult*zline\n",
        "#          if j < 6:\n",
        "#            print(\"xline: \", xline)\n",
        "#            print(\"zline: \", zline)\n",
        "\n",
        "#          print(xline)\n",
        "          xline_below = np.linspace((y_new[rnumber,2*j] - y_new[rnumber,2*j + 2*mdn_mix])* det_width, (y_new[rnumber,2*j+1] - y_new[rnumber,2*j + 2*mdn_mix+1]) * det_width, 100)\n",
        "          xline_above = np.linspace((y_new[rnumber,2*j] + y_new[rnumber,2*j + 2*mdn_mix])* det_width, (y_new[rnumber,2*j+1] + y_new[rnumber,2*j + 2*mdn_mix+1]) * det_width, 100)\n",
        "\n",
        "  \n",
        "  #          xline_above = (y_new[rnumber,2*j] + y_new[rnumber,2*j + 2*mdn_mix])*det_width + (y_new[rnumber,2*j+1] + y_new[rnumber,2*j + 2*mdn_mix+1])*slope_mult*zline\n",
        "    \n",
        "         # b1 = -b/a\n",
        "#          b1 = -(y_new[rnumber,2*j] * det_depth)/y_new[rnumber,2*j+1]\n",
        "          b1 = y_new[rnumber,2*j] * det_width\n",
        "          b2 = y_new[rnumber,2*j] * det_width+y_new[rnumber,2*j+1]*slope_mult*det_depth\n",
        "        # b2 = -(det_depth - b)/a\n",
        "#          b2 = -(det_depth - y_new[rnumber,2*j] * det_depth)/y_new[rnumber,2*j+1]\n",
        "         \n",
        "        \n",
        "#          print(\"a, b: \", y_new[rnumber,2*j+1], y_new[rnumber,2*j] * det_depth)\n",
        "#          print(\"b1, b2: \", b1, b2)\n",
        "\n",
        "        \n",
        "         #yline = y_new[rnumber,j,2]*det_width+y_new[rnumber,j,3]*slope_mult*zline  \n",
        "  ###       xline = 100*y_new[rnumber,k,0]+100/slope_scale*y_new[rnumber,k,1]*zline\n",
        "  ###       yline = 100*y_new[rnumber,k,2]+100/slope_scale*y_new[rnumber,k,3]*zline\n",
        "#          ax.plot(zline, xline, 'blue', alpha=0.5)\n",
        "          ax.plot(zline, xline_below,  'blue', alpha=0.2)\n",
        "          ax.plot(zline, xline_above,  'blue', alpha=0.2)\n",
        "          ax.fill_between(zline, xline_below, xline_above, alpha = 0.1, color='blue')\n",
        "      \n",
        "      \n",
        "         #Calculating Accuracy\n",
        "#          if(abs(y_new[rnumber,2*j] - y_test[rnumber,j,0]) < 3 * y_new[rnumber,2*j + 2*mdn_mix] and\n",
        "#             abs(y_new[rnumber,2*j+1] - y_test[rnumber,j,1]) < 3 * y_new[rnumber,2*j + 2*mdn_mix+1]):\n",
        "# #            xline2 = np.linspace(y_test[rnumber,j,0] * det_width, y_test[rnumber,j,1] * det_width, 100)\n",
        "# #            xline3 = np.linspace(y_new[rnumber,2*j] * det_width, y_new[rnumber,2*j+1] * det_width, 100)\n",
        "# #            xline_below = np.linspace((y_new[rnumber,2*j] - y_new[rnumber,2*j + 2*mdn_mix])* det_width, (y_new[rnumber,2*j+1] - y_new[rnumber,2*j + 2*mdn_mix+1]) * det_width, 100)\n",
        "\n",
        "#            total_accuracy_nominator += 1\n",
        "#          total_accuracy_denominator += 1\n",
        "    \n",
        "      #ax.view_init(elev=10., azim=0)\n",
        "      \n",
        "      ###ax.scatter([0,det_width],[0,det_depth], c= 'blue', s=0)\n",
        "\n",
        "      # set axes range\n",
        "      ax.set_xlim(0,det_depth+(canals-1)*canal_depth)\n",
        "      #ax.set_xlim3d(0,det_width)\n",
        "      ax.set_ylim(0,det_width)\n",
        "      #ax.axis('equal')\n",
        "      \n",
        "      \n",
        "\n",
        "      print('event_res_'+str(efficiency)+\"_\"+str(prob_noise)+\"_\"+str(k)+'.png')\n",
        "      plt.savefig('event_res_'+str(efficiency)+\"_\"+str(prob_noise)+\"_\"+str(k)+'.png')\n",
        "      \n",
        "#       ax.view_init(90)\n",
        "\n",
        "      \n",
        "      plt.show()\n",
        "#   print(\"Total accuracy = \", total_accuracy_nominator/total_accuracy_denominator * 100, \"%\")\n",
        "  \n",
        "################################################################################################\n",
        "\n",
        "\n",
        "def plot_predict_event2(y_new = y_new):\n",
        "\n",
        "  \n",
        "  total_accuracy_nominator = 0\n",
        "  total_accuracy_denominator = 0\n",
        "  #plot sample events with true and reconstructed tracks\n",
        "  print(\"Plot events with predicted tracks, efficiency, prob_noise = \",efficiency,prob_noise)\n",
        "  \n",
        "  print(\"y_new shape :: \",y_new.shape)\n",
        "  \n",
        "\n",
        "  for n in range(8):\n",
        "    for k in range(int(100*(prob_noise+0.10))):\n",
        "      rnumber = np.random.randint(0,len(x_test))\n",
        "      x,z = x_test[rnumber,0].nonzero()\n",
        "      \n",
        "      x+=canal_depth*(x//canal_det)\n",
        "      \n",
        "      from mpl_toolkits.mplot3d import Axes3D\n",
        "      fig = plt.figure()\n",
        "      ax = fig.add_subplot(111)\n",
        "      #ax.scatter(x, z, c= 'red', s=5, alpha=0.9)\n",
        "    \n",
        "      ax.set_xlabel('Z (depth)')\n",
        "      #ax.set_ylabel('Y')\n",
        "      ax.set_ylabel('X')\n",
        "  \n",
        "  \n",
        "   \n",
        "    \n",
        "      for j in range(mdn_mix):\n",
        "      # Data for a three-dimensional line\n",
        "         #print(\"index \",k)\n",
        "         #print(\"ytest \", y_test[rnumber])\n",
        "         zline = np.linspace(0, det_depth+canal_depth*(canals-1), 100)\n",
        "#          print(\"zline: \", zline)\n",
        "#          if j < 6:\n",
        "#           print(y_test.shape)\n",
        "\n",
        "\n",
        "#          xline = y_test[rnumber,j,0]*det_width+y_test[rnumber,j,1]*slope_mult*zline\n",
        "          # parametry b1, b2\n",
        "         if j < max_tracks:\n",
        "          xline2 = np.linspace(y_test[rnumber,j,0] * det_width, y_test[rnumber,j,1] * det_width, 100)\n",
        "          \n",
        "  \n",
        "         #yline = y_test[rnumber,j,2]*det_width+y_test[rnumber,j,3]*slope_mult*zline \n",
        "###       xline = 100*y_test[rnumber,k,0]+100/slope_scale*y_test[rnumber,k,1]*zline\n",
        "###       yline = 100*y_test[rnumber,k,2]+100/slope_scale*y_test[rnumber,k,3]*zline\n",
        "          ax.plot(zline, xline2, 'red', alpha=0.5)\n",
        "#          if j < 6:\n",
        "#           print(y_new.shape)\n",
        "#          xline = y_new[rnumber,2*j]*det_width+y_new[rnumber,2*j+1]*slope_mult*zline \n",
        "         #yline = y_new[rnumber,j,2]*det_width+y_new[rnumber,j,3]*slope_mult*zline  \n",
        "###       xline = 100*y_new[rnumber,k,0]+100/slope_scale*y_new[rnumber,k,1]*zline\n",
        "###       yline = 100*y_new[rnumber,k,2]+100/slope_scale*y_new[rnumber,k,3]*zline\n",
        "\n",
        "\n",
        "        # parametry b1, b2\n",
        "         if y_new[rnumber, j + 4*mdn_mix] > 0:\n",
        "          xline3 = np.linspace(y_new[rnumber,2*j] * det_width, y_new[rnumber,2*j+1] * det_width, 100)\n",
        "          #ax.plot(zline, xline3, 'blue', alpha=0.5)\n",
        "         \n",
        "  \n",
        "  \n",
        "#          print(y_new.shape)\n",
        "          #xline = y_new[rnumber,2*j]*det_width+y_new[rnumber,2*j+1]*slope_mult*zline\n",
        "#          if j < 6:\n",
        "#            print(\"xline: \", xline)\n",
        "#            print(\"zline: \", zline)\n",
        "\n",
        "#          print(xline)\n",
        "          xline_below = np.linspace((y_new[rnumber,2*j] - y_new[rnumber,2*j + 2*mdn_mix])* det_width, (y_new[rnumber,2*j+1] - y_new[rnumber,2*j + 2*mdn_mix+1]) * det_width, 100)\n",
        "          xline_above = np.linspace((y_new[rnumber,2*j] + y_new[rnumber,2*j + 2*mdn_mix])* det_width, (y_new[rnumber,2*j+1] + y_new[rnumber,2*j + 2*mdn_mix+1]) * det_width, 100)\n",
        "\n",
        "  \n",
        "  #          xline_above = (y_new[rnumber,2*j] + y_new[rnumber,2*j + 2*mdn_mix])*det_width + (y_new[rnumber,2*j+1] + y_new[rnumber,2*j + 2*mdn_mix+1])*slope_mult*zline\n",
        "    \n",
        "         # b1 = -b/a\n",
        "#          b1 = -(y_new[rnumber,2*j] * det_depth)/y_new[rnumber,2*j+1]\n",
        "          #b1 = y_new[rnumber,2*j] * det_width\n",
        "          #b2 = y_new[rnumber,2*j] * det_width+y_new[rnumber,2*j+1]*slope_mult*det_depth\n",
        "        # b2 = -(det_depth - b)/a\n",
        "#          b2 = -(det_depth - y_new[rnumber,2*j] * det_depth)/y_new[rnumber,2*j+1]\n",
        "         \n",
        "        \n",
        "#          print(\"a, b: \", y_new[rnumber,2*j+1], y_new[rnumber,2*j] * det_depth)\n",
        "#          print(\"b1, b2: \", b1, b2)\n",
        "\n",
        "        \n",
        "         #yline = y_new[rnumber,j,2]*det_width+y_new[rnumber,j,3]*slope_mult*zline  \n",
        "  ###       xline = 100*y_new[rnumber,k,0]+100/slope_scale*y_new[rnumber,k,1]*zline\n",
        "  ###       yline = 100*y_new[rnumber,k,2]+100/slope_scale*y_new[rnumber,k,3]*zline\n",
        "#          ax.plot(zline, xline, 'blue', alpha=0.5)\n",
        "          #ax.plot(zline, xline_below,  'blue', alpha=0.2)\n",
        "          #ax.plot(zline, xline_above,  'blue', alpha=0.2)\n",
        "          ax.fill_between(zline, xline_below, xline_above, alpha = 0.1, color='blue')\n",
        "      \n",
        "      \n",
        "         #Calculating Accuracy\n",
        "#          if(abs(y_new[rnumber,2*j] - y_test[rnumber,j,0]) < 3 * y_new[rnumber,2*j + 2*mdn_mix] and\n",
        "#             abs(y_new[rnumber,2*j+1] - y_test[rnumber,j,1]) < 3 * y_new[rnumber,2*j + 2*mdn_mix+1]):\n",
        "# #            xline2 = np.linspace(y_test[rnumber,j,0] * det_width, y_test[rnumber,j,1] * det_width, 100)\n",
        "# #            xline3 = np.linspace(y_new[rnumber,2*j] * det_width, y_new[rnumber,2*j+1] * det_width, 100)\n",
        "# #            xline_below = np.linspace((y_new[rnumber,2*j] - y_new[rnumber,2*j + 2*mdn_mix])* det_width, (y_new[rnumber,2*j+1] - y_new[rnumber,2*j + 2*mdn_mix+1]) * det_width, 100)\n",
        "\n",
        "#            total_accuracy_nominator += 1\n",
        "#          total_accuracy_denominator += 1\n",
        "    \n",
        "      #ax.view_init(elev=10., azim=0)\n",
        "      \n",
        "      ###ax.scatter([0,det_width],[0,det_depth], c= 'blue', s=0)\n",
        "\n",
        "      # set axes range\n",
        "      ax.set_xlim(0,det_depth+(canals-1)*canal_depth)\n",
        "      #ax.set_xlim3d(0,det_width)\n",
        "      ax.set_ylim(0,det_width)\n",
        "      #ax.axis('equal')\n",
        "      \n",
        "      \n",
        "\n",
        "      print('event_res_'+str(efficiency)+\"_\"+str(prob_noise)+\"_\"+str(k)+'.png')\n",
        "      plt.savefig('event_res_'+str(efficiency)+\"_\"+str(prob_noise)+\"_\"+str(k)+'.png')\n",
        "      \n",
        "#       ax.view_init(90)\n",
        "\n",
        "      \n",
        "      plt.show()\n",
        "#   print(\"Total accuracy = \", total_accuracy_nominator/total_accuracy_denominator * 100, \"%\")\n",
        "  \n",
        "\n",
        "#########################################################################################################\n",
        "\n",
        "\n",
        "def get_accuracy(y_new = y_new, sigmas = 3):\n",
        "  total_accuracy_nominator = 0\n",
        "  total_accuracy_denominator = 0\n",
        "  for rnumber in range(len(x_test)):\n",
        "    tmp_array = [el for el in y_new[rnumber, 4*mdn_mix:4*mdn_mix + mdn_mix]]\n",
        "\n",
        "    is_good = True\n",
        "#     for j in range(len(y_test[rnumber])):\n",
        "#     print(y_new[rnumber, 4*mdn_mix:4*mdn_mix + mdn_mix])\n",
        "    max_index = np.argmax(tmp_array)\n",
        "    sorted_indexes = np.argsort(tmp_array)\n",
        "#     print(max_index)\n",
        "\n",
        "    for j in range(len(y_test[rnumber])):\n",
        "      if(is_good and\n",
        "#           abs(y_new[rnumber,2*j] - y_test[rnumber,j,0]) < sigmas * y_new[rnumber,2*j + 2*mdn_mix] and\n",
        "#                 abs(y_new[rnumber,2*j+1] - y_test[rnumber,j,1]) < sigmas * y_new[rnumber,2*j + 2*mdn_mix+1]):\n",
        "                  \n",
        "          (abs(y_new[rnumber,2*sorted_indexes[-1]] - y_test[rnumber,j,0]) < sigmas * y_new[rnumber,2*sorted_indexes[-1] + 2*mdn_mix] and\n",
        "                          abs(y_new[rnumber,2*sorted_indexes[-1]+1] - y_test[rnumber,j,1]) < sigmas * y_new[rnumber,2*sorted_indexes[-1] + 2*mdn_mix+1]) or\n",
        "          (abs(y_new[rnumber,2*sorted_indexes[-2]] - y_test[rnumber,j,0]) < sigmas * y_new[rnumber,2*sorted_indexes[-2] + 2*mdn_mix] and\n",
        "                          abs(y_new[rnumber,2*sorted_indexes[-2]+1] - y_test[rnumber,j,1]) < sigmas * y_new[rnumber,2*sorted_indexes[-2] + 2*mdn_mix+1])):\n",
        "\n",
        "    #            xline2 = np.linspace(y_test[rnumber,j,0] * det_width, y_test[rnumber,j,1] * det_width, 100)\n",
        "    #            xline3 = np.linspace(y_new[rnumber,2*j] * det_width, y_new[rnumber,2*j+1] * det_width, 100)\n",
        "    #            xline_below = np.linspace((y_new[rnumber,2*j] - y_new[rnumber,2*j + 2*mdn_mix])* det_width, (y_new[rnumber,2*j+1] - y_new[rnumber,2*j + 2*mdn_mix+1]) * det_width, 100)\n",
        "        total_accuracy_nominator += 1\n",
        "      total_accuracy_denominator += 1\n",
        "#       print(\"y_test[rnumber]: \", y_test[rnumber], y_new[rnumber])\n",
        "    \n",
        "  print(\"Total accuracy = \", total_accuracy_nominator/total_accuracy_denominator * 100, \"%\")\n",
        "  return total_accuracy_nominator/total_accuracy_denominator * 100\n",
        "\n",
        "#get_accuracy(y_new, 1)\n",
        "  \n",
        "  \n",
        "\n",
        "#########################################################################################################\n",
        "\n",
        "def get_std(y_new = y_new):\n",
        "  std_nominator = np.zeros(2)\n",
        "  std_denominator = 0.0\n",
        "  for rnumber in range(len(x_test)):\n",
        "#     for j in range(len(y_test[rnumber])):\n",
        "    for j in range(mdn_mix):\n",
        "\n",
        "      if(y_new[rnumber, j + 4*mdn_mix] > 0):\n",
        "        std_nominator += np.asarray([y_new[rnumber,2*j + 2*mdn_mix], y_new[rnumber,2*j + 2*mdn_mix+1]])\n",
        "    #            xline2 = np.linspace(y_test[rnumber,j,0] * det_width, y_test[rnumber,j,1] * det_width, 100)\n",
        "    #            xline3 = np.linspace(y_new[rnumber,2*j] * det_width, y_new[rnumber,2*j+1] * det_width, 100)\n",
        "    #            xline_below = np.linspace((y_new[rnumber,2*j] - y_new[rnumber,2*j + 2*mdn_mix])* det_width, (y_new[rnumber,2*j+1] - y_new[rnumber,2*j + 2*mdn_mix+1]) * det_width, 100)\n",
        "        std_denominator += 1.0\n",
        "#       print(\"y_test[rnumber]: \", y_test[rnumber], y_new[rnumber])\n",
        "    \n",
        "  print(\"Avg_std = \", np.asarray(std_nominator) / std_denominator)\n",
        "  return np.asarray(std_nominator) / std_denominator\n",
        "  \n",
        "#get_std(y_new)\n",
        "\n",
        "################################################################################################################\n",
        "\n",
        "def write_predict(y_new = y_new):\n",
        "\n",
        "  print(\"Write output data, efficiency, prob_noise = \",efficiency,prob_noise)\n",
        "\n",
        "  print(\"y_new shape :: \",y_new.shape)\n",
        "  \n",
        "\n",
        "  # write the output - true and reconstructed tracks\t\n",
        "  print(\"Writing dnn_output.txt by printing y_new and y_test: \",y_new.shape)  \n",
        "  f= open(\"dnn_output\"+str(efficiency)+\"_\"+str(prob_noise)+\".txt\",\"w+\")\n",
        "  for i in range(len(y_test)):\n",
        "    k = len(y_test[i])\n",
        "    f.write(\"%s \\n\" % k)\n",
        "    for j in range(len(y_test[i])):\n",
        "      for l in range(len(y_test[i][j])):\n",
        "        f.write(\"%s \" % y_test[i][j][l])\n",
        "      f.write(\"\\n\")\n",
        "    for j in range(len(y_new[i])):\n",
        "      for l in range(len(y_new[i][j])):\n",
        "        f.write(\"%s \" % y_new[i][j][l])      \n",
        "      f.write(\"\\n\")\n",
        "    \n",
        "  f.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z02_bQlZsAu-",
        "colab_type": "code",
        "outputId": "683bf61f-5d4b-45b3-9336-13767280a6d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "source": [
        "\n",
        "# the data, split between train and test sets\n",
        "#(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "###x_train, y_train = gen_n_tracks(batch_size=batch_size).__next__()\n",
        "\n",
        "x_test, y_test = gen_n_tracks(batch_size=batch_size, n_tracks=mean_tracks, train = True).__next__()\n",
        "\n",
        "###y_train = y_train\n",
        "###y_test = y_test\n",
        "\n",
        "# add noise\n",
        "###noise_train = gen_noise(batch_size=batch_size).__next__()\n",
        "\n",
        "noise_test = gen_noise(batch_size=batch_size, prob_noise=prob_noise, dims = 2).__next__()\n",
        "\n",
        "###x_train = x_train+noise_train\n",
        "x_test  = x_test+noise_test\n",
        "\n",
        "\n",
        "###print(\"x_train \",x_train.shape)i\n",
        "###print(\"y_train \",y_train.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(x_train[0,0])\n",
        "# print(\"Train targets \",y_train[0])\n",
        "plt.imshow(x_test[0,0])\n",
        "print(\"Test targets \",y_test[0])\n",
        "\n",
        "\n",
        "#\n",
        "# if K.image_data_format() == 'channels_first':\n",
        "#     x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "#     x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "#     y_train = y_train.reshape(y_train.shape[0], 1, 2)\n",
        "#     y_test = y_test.reshape(y_test.shape[0], 1, 2)\n",
        "#     input_shape = (1, img_rows, img_cols)\n",
        "# else:\n",
        "#     x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "#     x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "#     y_train = y_train.reshape(y_train.shape[0], 2, 1)\n",
        "#     y_test = y_test.reshape(y_test.shape[0], 2, 1)\n",
        "#     input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "###x_train = x_train.astype('int')\n",
        "\n",
        "x_test = x_test.astype('int')\n",
        "###x_train = np.clip(x_train, 0, 1)\n",
        "\n",
        "x_test = np.clip(x_test, 0, 1)\n",
        "#x_train /= 255\n",
        "#x_test /= 255\n",
        "#print('x_train shape:', x_train.shape)\n",
        "#print(x_train.shape[0], 'train samples')\n",
        "#print(x_test.shape[0], 'test samples')\n",
        "#print('y_train shape:', y_train.shape)\n",
        "#print(y_train.shape[0], 'train samples')\n",
        "#print(y_test.shape[0], 'test samples')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test targets  [[0.9760271  0.40265994]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAAtCAYAAACgXKBrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz-ZxGnuALZn",
        "colab_type": "code",
        "outputId": "9c7343e7-27f1-4f40-a193-fd62dfa7e4b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#printing TPU information\n",
        "\n",
        "if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "  print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')\n",
        "else:\n",
        "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  print ('TPU address is', tpu_address)\n",
        "\n",
        "  with tf.Session(tpu_address) as session:\n",
        "    devices = session.list_devices()\n",
        "    \n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(devices)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "69VjCyS0xBLa"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG4dJAfDXpZU",
        "colab_type": "code",
        "outputId": "7c68d3ba-c58d-4cdc-daa6-a623a46c89b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "start_training_time = time.time()\n",
        "\n",
        "# if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "\n",
        "#   model = Sequential()\n",
        "#   input_shape=(1,det_width, det_width, det_depth)\n",
        "\n",
        "#   print(\"input_shape \",input_shape)\n",
        "\n",
        "#   model.add(Reshape((det_width, det_width, det_depth,1),\n",
        "#                                   input_shape=input_shape))\n",
        "#   model.add(Conv3D(32, (3, 3, 3), activation='relu'))  #8\n",
        "#   model.add(Conv3D(32, (3, 3, 3), activation='relu'))  #8\n",
        "# #  model.add(Conv3D(32, (3, 3, 3), activation='relu'))  #added\n",
        "#   model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "#   model.add(Conv3D(128, (3, 3, 3), activation='relu'))  #32\n",
        "#   model.add(Conv3D(128, (3, 3, 3), activation='relu'))  #32\n",
        "# #  model.add(Conv3D(128, (3, 3, 3), activation='relu'))  #added\n",
        "#   model.add(Dropout(0.25))\n",
        "#   model.add(Flatten())\n",
        "#   model.add(Dense(400, activation='relu'))  # 400\n",
        "# #  model.add(Dense(400))  # added\n",
        "#   #model.add(Dropout(0.5))\n",
        "#   #model.add(Dense(2, activation='tanh'))\n",
        "\n",
        "\n",
        "#   model.add(RepeatVector(max_tracks))\n",
        "#   model.add(LSTM(400, return_sequences=True))\n",
        "#   model.add(TimeDistributed(Dense(4)))\n",
        "\n",
        "#   # model.add(Flatten(input_shape=input_shape))\n",
        "#   # #model.add(Reshape(input_shape - (1, ), input_shape=input_shape))\n",
        "#   # model.add(Dense(32, activation='relu'))\n",
        "#   # model.add(Dense(32, activation='relu'))\n",
        "#   # model.add(Dense(32, activation='relu'))\n",
        "#   # model.add(Dense(32, activation='relu'))\n",
        "#   # #model.add(Dense(32, activation='relu'))\n",
        "#   # #model.add(Dense(32, activation='relu'))\n",
        "#   # model.add(Dropout(0.5))\n",
        "#   # model.add(Dense(2, activation='tanh'))\n",
        "\n",
        "\n",
        "#   #model.add(Reshape((1, 2),input_shape=(2,)))\n",
        "#   model.add(Reshape((max_tracks, 4),input_shape=(4*max_tracks,)))\n",
        "\n",
        "# else:\n",
        "  ###stara sieÄ###\n",
        "#   model = tf.keras.layers.Sequential()\n",
        "#   input_shape=(1,det_width, det_width, det_depth)\n",
        "\n",
        "#   print(\"input_shape \",input_shape)\n",
        "\n",
        "#   model.add(Reshape((det_width, det_width, det_depth,1),\n",
        "#                                   input_shape=input_shape))\n",
        "#   model.add(Conv3D(32, (3, 3, 3), activation='relu'))  #8\n",
        "#   model.add(tf.keras.layers.Conv3D(32, (3, 3, 3), activation='relu'))  #8\n",
        "#   model.add(tf.keras.layers.Conv3D(32, (3, 3, 3), activation='relu'))  #added\n",
        "#   model.add(tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "#   model.add(tf.keras.layers.Conv3D(128, (3, 3, 3), activation='relu'))  #32\n",
        "#   model.add(tf.keras.layers.Conv3D(128, (3, 3, 3), activation='relu'))  #32\n",
        "#   model.add(tf.keras.layers.Conv3D(128, (3, 3, 3), activation='relu'))  #added\n",
        "#   model.add(tf.keras.layers.Dropout(0.25))\n",
        "#   model.add(tf.keras.layers.Flatten())\n",
        "#   model.add(tf.keras.layers.Dense(400, activation='relu'))  # 400\n",
        "#   model.add(tf.keras.layers.Dense(400))  # added\n",
        "#   #model.add(Dropout(0.5))\n",
        "#   #model.add(Dense(2, activation='tanh'))\n",
        "\n",
        "\n",
        "#   model.add(tf.keras.layers.RepeatVector(max_tracks))\n",
        "#   model.add(tf.keras.layers.LSTM(400, return_sequences=True))\n",
        "#   model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(4)))\n",
        "\n",
        "#   # model.add(Flatten(input_shape=input_shape))\n",
        "#   # #model.add(Reshape(input_shape - (1, ), input_shape=input_shape))\n",
        "#   # model.add(Dense(32, activation='relu'))\n",
        "#   # model.add(Dense(32, activation='relu'))\n",
        "#   # model.add(Dense(32, activation='relu'))\n",
        "#   # model.add(Dense(32, activation='relu'))\n",
        "#   # #model.add(Dense(32, activation='relu'))\n",
        "#   # #model.add(Dense(32, activation='relu'))\n",
        "#   # model.add(Dropout(0.5))\n",
        "#   # model.add(Dense(2, activation='tanh'))\n",
        "\n",
        "\n",
        "#   #model.add(Reshape((1, 2),input_shape=(2,)))\n",
        "#   model.add(tf.keras.layers.Reshape((max_tracks, 4),input_shape=(4*max_tracks,)))  \n",
        "\n",
        "    ###zmiana na 2D###\n",
        "\n",
        "model = Sequential()\n",
        "input_shape=(1,det_depth, det_width)\n",
        "\n",
        "print(\"input_shape \",input_shape)\n",
        "\n",
        "model.add(Reshape((det_depth, det_width,1),\n",
        "                                input_shape=input_shape))\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))  #8\n",
        "model.add(Conv2D(32, (3, 63), activation='relu'))  #8\n",
        "model.add(Conv2D(64, (3, 9), activation='relu'))  #added\n",
        "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))  #32\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))  #32\n",
        "#model.add(Conv2D(128, (3, 3), activation='relu'))  #added\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(400, activation='relu'))  # 400\n",
        "#model.add(Dense(400))\n",
        "model.add(Dense(400))  # added\n",
        "model.add(Dense(400))\n",
        "#model.add(Dropout(0.5))\n",
        "#model.add(Dense(2, activation='tanh'))\n",
        "\n",
        "\n",
        "#model.add(RepeatVector(max_tracks))\n",
        "#model.add(LSTM(400, return_sequences=True))\n",
        "# model.add(TimeDistributed(Dense(2)))\n",
        "#model.add(TimeDistributed(mdn.MDN(mdn_out,mdn_mix)))\n",
        "\n",
        "model.add(mdn.MDN(mdn_out,mdn_mix))\n",
        "\n",
        "# model.add(Flatten(input_shape=input_shape))\n",
        "# #model.add(Reshape(input_shape - (1, ), input_shape=input_shape))\n",
        "# model.add(Dense(32, activation='relu'))\n",
        "# model.add(Dense(32, activation='relu'))\n",
        "# model.add(Dense(32, activation='relu'))\n",
        "# model.add(Dense(32, activation='relu'))\n",
        "# #model.add(Dense(32, activation='relu'))\n",
        "# #model.add(Dense(32, activation='relu'))\n",
        "# model.add(Dropout(0.5))\n",
        "# model.add(Dense(2, activation='tanh'))\n",
        "\n",
        "\n",
        "#model.add(Reshape((1, 2),input_shape=(2,)))\n",
        "#model.add(Reshape((max_tracks, 2),input_shape=(2*max_tracks,)))  \n",
        "\n",
        "  \n",
        "  ####koniec####\n",
        "  \n",
        "\n",
        "# print outputs for each layer\n",
        "outputs =  [layer.output for layer in model.layers]\n",
        "print (outputs)\n",
        "print (model.summary())\n",
        "\n",
        "print (\"plotting model\")\n",
        "plot_model(model, to_file='model.png',rankdir='LR',show_shapes=False,show_layer_names=False )  ###, show_shapes=True\n",
        "\n",
        "print (\"Showing model\")\n",
        "from IPython.display import Image\n",
        "display(Image(filename='model.png'))\n",
        "!ls\n",
        "print (\"Model shown\")\n",
        "\n",
        "#sys.exit(\"Stopping here\")\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "    Converts a Keras model to dot format and save to a file.\n",
        "    # Arguments\n",
        "        model: A Keras model instance\n",
        "        to_file: File name of the plot image.\n",
        "        show_shapes: whether to display shape information.\n",
        "        show_layer_names: whether to display layer names.\n",
        "        rankdir: `rankdir` argument passed to PyDot,\n",
        "            a string specifying the format of the plot:\n",
        "            'TB' creates a vertical plot;\n",
        "            'LR' creates a horizontal plot.\n",
        "        expand_nested: whether to expand nested models into clusters.\n",
        "        dpi: dot DPI.\n",
        "'''\n",
        "\n",
        "# Vizualizing model structure\n",
        "\n",
        "sequential_model_to_ascii_printout(model)\n",
        "\n",
        "\n",
        "#history=model.fit(x_train, y_train,\n",
        "#          batch_size=batch_size_NN,\n",
        "#          epochs=epochs,\n",
        "#          verbose=1,\n",
        "#          validation_data=(x_test, y_test))\n",
        "\n",
        "\n",
        "#################Oryginalny data_generator################\n",
        "\n",
        "\n",
        "# def data_generator(batch_size_gen=batch_size_NN,prob_noise=prob_noise,n_tracks=mean_tracks):\n",
        "  \n",
        "#   while True: \n",
        "#    # the data, split between train and test sets\n",
        "#    #(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "#     x_train, y_train = gen_n_tracks(batch_size=batch_size_gen,n_tracks=mean_tracks).__next__()\n",
        "    \n",
        "\n",
        "#      #y_train = y_train/100\n",
        "\n",
        "#      # add noise\n",
        "#     noise_train = gen_noise(batch_size=batch_size_gen,prob_noise=prob_noise).__next__()\n",
        "#     x_train = x_train+noise_train\n",
        "\n",
        "#      #print(\"x_train: \",x_train.shape)\n",
        "#      #print(\"y_train: \",y_train.shape)\n",
        "#     yield x_train, y_train\n",
        "   \n",
        "#   #print(\"generator \",data_generator())\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0801 11:55:15.827084 140040865830784 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0801 11:55:15.880986 140040865830784 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "input_shape  (1, 12, 400)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0801 11:55:15.903209 140040865830784 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0801 11:55:15.994094 140040865830784 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0801 11:55:16.006209 140040865830784 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor 'reshape_1/Reshape:0' shape=(?, 12, 400, 1) dtype=float32>, <tf.Tensor 'conv2d_1/Relu:0' shape=(?, 10, 398, 32) dtype=float32>, <tf.Tensor 'conv2d_2/Relu:0' shape=(?, 8, 336, 32) dtype=float32>, <tf.Tensor 'conv2d_3/Relu:0' shape=(?, 6, 328, 64) dtype=float32>, <tf.Tensor 'conv2d_4/Relu:0' shape=(?, 4, 326, 128) dtype=float32>, <tf.Tensor 'conv2d_5/Relu:0' shape=(?, 2, 324, 128) dtype=float32>, <tf.Tensor 'dropout_1/cond/Merge:0' shape=(?, 2, 324, 128) dtype=float32>, <tf.Tensor 'flatten_1/Reshape:0' shape=(?, ?) dtype=float32>, <tf.Tensor 'dense_1/Relu:0' shape=(?, 400) dtype=float32>, <tf.Tensor 'dense_2/BiasAdd:0' shape=(?, 400) dtype=float32>, <tf.Tensor 'dense_3/BiasAdd:0' shape=(?, 400) dtype=float32>, <tf.Tensor 'mdn_1/MDN/mdn_outputs/concat:0' shape=(?, 15) dtype=float32>]\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_1 (Reshape)          (None, 12, 400, 1)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 10, 398, 32)       320       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 8, 336, 32)        193568    \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 6, 328, 64)        55360     \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 4, 326, 128)       73856     \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 2, 324, 128)       147584    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 2, 324, 128)       0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 82944)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 400)               33178000  \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 400)               160400    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 400)               160400    \n",
            "_________________________________________________________________\n",
            "mdn_1 (MDN)                  (None, 15)                6015      \n",
            "=================================================================\n",
            "Total params: 33,975,503\n",
            "Trainable params: 33,975,503\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "plotting model\n",
            "Showing model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABs0AAAA8CAIAAACipWPBAAAABmJLR0QA/wD/AP+gvaeTAAAgAElE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\nSUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "model.png  sample_data\n",
            "Model shown\n",
            "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
            "\n",
            "               Input   #####      1   12  400\n",
            "             Reshape     |   -------------------         0     0.0%\n",
            "                       #####     12  400    1\n",
            "              Conv2D    \\|/  -------------------       320     0.0%\n",
            "                relu   #####     10  398   32\n",
            "              Conv2D    \\|/  -------------------    193568     0.6%\n",
            "                relu   #####      8  336   32\n",
            "              Conv2D    \\|/  -------------------     55360     0.2%\n",
            "                relu   #####      6  328   64\n",
            "              Conv2D    \\|/  -------------------     73856     0.2%\n",
            "                relu   #####      4  326  128\n",
            "              Conv2D    \\|/  -------------------    147584     0.4%\n",
            "                relu   #####      2  324  128\n",
            "             Dropout    | || -------------------         0     0.0%\n",
            "                       #####      2  324  128\n",
            "             Flatten   ||||| -------------------         0     0.0%\n",
            "                       #####       82944\n",
            "               Dense   XXXXX -------------------  33178000    97.7%\n",
            "                relu   #####         400\n",
            "               Dense   XXXXX -------------------    160400     0.5%\n",
            "                       #####         400\n",
            "               Dense   XXXXX -------------------    160400     0.5%\n",
            "                       #####         400\n",
            "                 MDN   ????? -------------------      6015     0.0%\n",
            "                       #####          15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZDXGSZF7ImY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_generator(batch_size_gen=batch_size_NN,prob_noise=prob_noise,n_tracks=mean_tracks):\n",
        "  \n",
        "  while True: \n",
        "   # the data, split between train and test sets\n",
        "    x_train, y_train = gen_n_tracks(batch_size=batch_size_gen,n_tracks=n_tracks, train = True).__next__()\n",
        "    #y_train = y_train[:, :,]\n",
        "\n",
        "     # add noise\n",
        "    noise_train = gen_noise(batch_size=batch_size_gen,prob_noise=prob_noise, dims = 2).__next__()\n",
        "    x_train = x_train+noise_train\n",
        "    \n",
        "    x_train = np.clip(x_train, 0, 1)\n",
        "#     print(x_train.shape)\n",
        "    \n",
        "#     fig = plt.figure()\n",
        "#     ax = fig.add_subplot(111)\n",
        "#     x, y = x_train[0,0,:,:]\n",
        "#     ax.scatter(x, y)\n",
        "#     plt.show()\n",
        "    \n",
        "#     plt.imshow(np.asarray(x_train[0,0,:,:]))\n",
        "   \n",
        "     #print(\"x_train: \",x_train.shape)\n",
        "     #print(\"y_train: \",y_train.shape)\n",
        "    yield x_train, y_train\n",
        "   \n",
        "\n",
        "dg = data_generator(batch_size_gen=2, prob_noise = 0.05)\n",
        "x, y = dg.__next__()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfPkKytjE0DN",
        "colab_type": "code",
        "outputId": "b094aa38-c56b-4fed-9f47-2793b1d38eb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "\n",
        "\n",
        "#using TPU model!!!\n",
        "# This address identifies the TPU we'll use when configuring TensorFlow.\n",
        "\n",
        "\n",
        "# if 'COLAB_TPU_ADDR'  in os.environ:  \n",
        "#   TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "#   tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "#   model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "#      model,\n",
        "#      strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "#         tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
        "\n",
        "#   model.compile(optimizer=tf.train.AdamOptimizer(), loss='mse')\n",
        "# else:\n",
        "\n",
        "\n",
        "#  model.compile(\n",
        "#      optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, \n",
        "#                                      epsilon=1e-8), loss='mse')\n",
        "#  rms = RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.1)\n",
        "#  model.compile(optimizer=rms, loss='mse')\n",
        "#  model.compile(optimizer='Adam', loss='mean_squared_error')\n",
        "  model.compile(optimizer='Adam', loss=mdn.get_mixture_loss_func(mdn_out,mdn_mix))\n",
        "\n",
        "#model.compile(optimizer=tf.train.AdamOptimizer(), loss='mse')\n",
        "#model.compile(loss='mean_squared_error', optimizer='Adam')\n",
        "\n",
        "#model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "#              optimizer=keras.optimizers.Adadelta(),\n",
        "#              metrics=['accuracy'])\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0801 11:55:18.149363 140040865830784 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0801 11:55:18.288892 140040865830784 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/distributions/mixture.py:154: Categorical.event_size (from tensorflow_probability.python.distributions.categorical) is deprecated and will be removed after 2019-05-19.\n",
            "Instructions for updating:\n",
            "The `event_size` property is deprecated.  Use `num_categories` instead.  They have the same value, but `event_size` is misnamed.\n",
            "W0801 11:55:18.438022 140040865830784 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:2403: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3PucksX_9Kt",
        "colab_type": "code",
        "outputId": "4fce1dab-2fe9-4ac0-8a55-a570e9166b58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''\n",
        "Warning \n",
        "if network learns to only fit one mixture to both tracks, it has gone into the minimum, training the network on larger noise and switching back\n",
        "with to lower one with pretrained network may help\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "#del model\n",
        "#model = load_model('partly_trained.h5')\n",
        "#prob_noise=0.005\n",
        "# patient early stopping\n",
        "es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=12)  # 'val_loss'\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='loss', mode='min', verbose=1,\n",
        "                     save_best_only=True)  # 'val_loss'\n",
        "\n",
        "#train with fit_generator - generate events on flight\n",
        "# pre-training with rms optimizer \n",
        "history=model.fit_generator(data_generator(prob_noise=prob_noise, n_tracks=mean_tracks),\n",
        "                            steps_per_epoch=steps_per_epoch,\n",
        "                            #validation_data=data_generator(), \n",
        "                            #validation_steps=steps_per_epoch,\n",
        "                            epochs = 64 #pre_epochs\n",
        "                            #,callbacks=[es\n",
        "                                        #, mc]\n",
        "                            #          ]\n",
        "                           )\n",
        "\n",
        "x_test, y_test = generate_testdata(prob_noise)\n",
        "#plot_sample()\n",
        "  \n",
        "y_new = predict(model)\n",
        "print(\"y_new shape: \",y_new.shape)\n",
        "plot_predict(y_new)\n",
        "plot_predict_event(y_new)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0801 11:55:19.355682 140040865830784 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/64\n",
            "16/16 [==============================] - 31s 2s/step - loss: 41.7710\n",
            "Epoch 2/64\n",
            "16/16 [==============================] - 18s 1s/step - loss: 0.7083\n",
            "Epoch 3/64\n",
            "16/16 [==============================] - 18s 1s/step - loss: 0.2257\n",
            "Epoch 4/64\n",
            "16/16 [==============================] - 18s 1s/step - loss: 0.1123\n",
            "Epoch 5/64\n",
            "16/16 [==============================] - 18s 1s/step - loss: 0.0754\n",
            "Epoch 6/64\n",
            "16/16 [==============================] - 18s 1s/step - loss: -0.0279\n",
            "Epoch 7/64\n",
            "16/16 [==============================] - 18s 1s/step - loss: -0.0773\n",
            "Epoch 8/64\n",
            "16/16 [==============================] - 18s 1s/step - loss: -0.1556\n",
            "Epoch 9/64\n",
            "16/16 [==============================] - 18s 1s/step - loss: -0.1894\n",
            "Epoch 10/64\n",
            "16/16 [==============================] - 18s 1s/step - loss: -0.0219\n",
            "Epoch 11/64\n",
            "16/16 [==============================] - 18s 1s/step - loss: -0.1499\n",
            "Epoch 12/64\n",
            "16/16 [==============================] - 18s 1s/step - loss: -0.3476\n",
            "Epoch 13/64\n",
            "16/16 [==============================] - 18s 1s/step - loss: -0.4103\n",
            "Epoch 14/64\n",
            "16/16 [==============================] - 18s 1s/step - loss: -0.4144\n",
            "Epoch 15/64\n",
            "16/16 [==============================] - 18s 1s/step - loss: -0.5447\n",
            "Epoch 16/64\n",
            "16/16 [==============================] - 18s 1s/step - loss: -0.5558\n",
            "Epoch 17/64\n",
            " 7/16 [============>.................] - ETA: 10s - loss: -0.5518"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-00d7f934d5d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                             \u001b[0;31m#validation_data=data_generator(),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                             \u001b[0;31m#validation_steps=steps_per_epoch,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                             \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m \u001b[0;31m#pre_epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                             \u001b[0;31m#,callbacks=[es\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                                         \u001b[0;31m#, mc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lele_fq5sr-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prob_noise=0.005\n",
        "# x_test, y_test = generate_testdata(prob_noise)\n",
        "# #plot_sample()\n",
        "\n",
        "# y_new = predict(model)\n",
        "# get_std(y_new)\n",
        "# get_accuracy(y_new)\n",
        "# print(\"y_new shape: \",y_new.shape)\n",
        "# plot_predict(y_new)\n",
        "# plot_predict_event(y_new)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWETD96bsqpG",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijQGyUFDGKn2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# serialize weights to HDF5\n",
        "model.save_weights(f\"model_{img_rows}_{img_cols}_{prob_noise}.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1HtNUv2HSIB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(f\"model_{img_rows}_{img_cols}_{prob_noise}.h5\")\n",
        "print(\"Model loaded from disk\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZNN42Wjd07s",
        "colab_type": "code",
        "outputId": "05fa20f8-b2b8-4335-dd0f-491b0c669c1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        }
      },
      "source": [
        "# summarize history for loss\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.rc('font',size=20)\n",
        "plt.plot(history.history['loss'])\n",
        "###plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.xlim(1, None)\n",
        "plt.ylim(None,1)\n",
        "#plt.ylim(ymax = 1.2*history.history['loss'][min(len(history.history['loss']),3)-1], ymin = 0)\n",
        "print(\"history\",history.history['loss'])\n",
        "#plt.rc('font',size=50)\n",
        "auxName = 'pretraining_'+str(efficiency)+'_'+str(prob_noise)+'.png'\n",
        "plt.savefig(auxName)\n",
        "  \n",
        "plt.show()\n",
        "#plt.savefig('training.png')\n",
        "plt.clf()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-a7c77d95992e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'font'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m###plt.plot(history.history['val_loss'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x720 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQ_ceZYG-bU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x_test, y_test = generate_testdata(prob_noise)\n",
        "# #plot_sample()\n",
        "  \n",
        "# y_new = predict(model)\n",
        "# print(\"y_new shape: \",y_new.shape)\n",
        "# # plot_predict(y_new)\n",
        "# plot_predict_event2(y_new)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCpjosf_Izxk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgG2ckJnJAsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPIeVktJf8up",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liz8jKSPXag8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# print(\"number of tracks \",mean_tracks)\n",
        "\n",
        "\n",
        "\n",
        "# for prob_noise in [0, 0.050, 0.100, 0.150, 0.200]:\n",
        "# #for prob_noise in [0.001, 0.004]:\n",
        "  \n",
        "#   train_model(model, prob_noise,epochs)\n",
        "  \n",
        "\n",
        "#   x_test, y_test = generate_testdata(prob_noise)\n",
        "#   plot_sample()\n",
        "  \n",
        "#   y_new = predict(model)\n",
        "#   print(\"y_new shape: \",y_new.shape)\n",
        "#   plot_predict(y_new)\n",
        "#   plot_predict_event(y_new)\n",
        "  \n",
        "#   write_predict(y_new)\n",
        "  \n",
        "#   !zip  /content/dnn_output_0.zip /content/*.png /content/*.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f7jq4sOJWU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# file=open('acc.txt','w+')\n",
        "# file2=open('std.txt','w+')\n",
        "\n",
        "# print(\"number of tracks \",mean_tracks)\n",
        "\n",
        "# accuracy_tab = []\n",
        "# avg_std_tab = []\n",
        "# tmp=[0, 0.050, 0.100, 0.150, 0.200,0.25,0.3,0.35,0.4]\n",
        "# for prob_noise in [i*28/400for i in tmp]:\n",
        "# #for prob_noise in [0.001, 0.004]:\n",
        "#   tmp_acc=[]\n",
        "#   tmp_std=[]\n",
        "# #   train_model(model, prob_noise, epochs)\n",
        "#   for i in range(5):\n",
        "#     train_model(model, prob_noise, 64)\n",
        "\n",
        "\n",
        "#     x_test, y_test = generate_testdata(prob_noise)\n",
        "  \n",
        "# #   plot_sample()\n",
        "  \n",
        "#     y_new = predict(model)\n",
        "# #   print(\"y_new shape: \",y_new.shape)\n",
        "# #   plot_predict(y_new)\n",
        "# #   plot_predict_event(y_new)\n",
        "  \n",
        "#     tmp_acc.append(get_accuracy(y_new = y_new, sigmas = 3))\n",
        "#     tmp_std.append(get_std(y_new = y_new))\n",
        "  \n",
        "# #   write_predict(y_new)\n",
        "  \n",
        "#   #!zip  /content/dnn_output_0.zip /content/*.png /content/*.txt\n",
        "#   accuracy_tab.append(tmp_acc)\n",
        "#   avg_std_tab.append(tmp_std)\n",
        "  \n",
        "#   print(\"accuracy_tab: \", accuracy_tab)\n",
        "#   print(\"avg_std_tab: \", avg_std_tab)\n",
        "  \n",
        "#   file.write(str(accuracy_tab))\n",
        "#   file2.write(str(avg_std_tab))\n",
        "  \n",
        "# file.close()\n",
        "# file2.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOzwDrG4aKcY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# end_training_time = time.time()\n",
        "\n",
        "\n",
        "\n",
        "# !zip  /content/dnn_output_final.zip /content/*.png /content/*.txt\n",
        "\n",
        "\n",
        "\n",
        "# print (\"    \")\n",
        "# print (\"###########################################################\")\n",
        "# print (\"    \")\n",
        "# print (\"CPU usage: \")\n",
        "# print (\"Deep NN training:      \", end_training_time-start_training_time)\n",
        "# #print (\"Pattern recognition:   \", end_pattern_time-start_pattern_time)\n",
        "# #print (\"Track fitting:         \", end_fit_time-start_fit_time)\n",
        "# print (\"    \")\n",
        "# print (\"###########################################################\")\n",
        "# print (\"    \")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5tUif94rT4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLp3N32KrWqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzLdGcLnPqh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Sj82lzE7nWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}